import { springAIClient } from './client';
import type { 
  ChatRequest, 
  ChatResponse, 
  ChatStreamChunk
} from '../../lib/spring-ai/types';

/**
 * Spring AI ì±„íŒ… API ì—”ë“œí¬ì¸íŠ¸
 */
export const chatEndpoints = {
  /**
   * ì±„íŒ… ì™„ì„± ìš”ì²­ (non-streaming)
   */
  async complete(request: ChatRequest): Promise<ChatResponse> {
    try {
      // Spring AI ë°±ì—”ë“œê°€ ì¤€ë¹„ë˜ê¸° ì „ê¹Œì§€ ëª¨ì˜ ì‘ë‹µ
      return await mockChatComplete(request);
      
      // ì‹¤ì œ êµ¬í˜„ (ë°±ì—”ë“œ ì¤€ë¹„ ì‹œ í™œì„±í™”)
      // return await springAIClient.post<ChatResponse>('/chat/completions', {
      //   model: request.options.model || 'gpt-3.5-turbo',
      //   messages: request.messages,
      //   temperature: request.options.temperature,
      //   max_tokens: request.options.maxTokens,
      //   top_p: request.options.topP,
      //   frequency_penalty: request.options.frequencyPenalty,
      //   presence_penalty: request.options.presencePenalty,
      //   stop: request.options.stop,
      //   stream: false
      // });
    } catch (error) {
      console.error('Chat completion failed:', error);
      throw error;
    }
  },

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì™„ì„± ìš”ì²­
   */
  async *stream(request: ChatRequest): AsyncIterableIterator<ChatStreamChunk> {
    try {
      // Spring AI ë°±ì—”ë“œê°€ ì¤€ë¹„ë˜ê¸° ì „ê¹Œì§€ ëª¨ì˜ ìŠ¤íŠ¸ë¦¬ë°
      yield* mockChatStream(request);
      return;
      
      // ì‹¤ì œ êµ¬í˜„ (ë°±ì—”ë“œ ì¤€ë¹„ ì‹œ í™œì„±í™”)
      // yield* springAIClient.stream<ChatStreamChunk>('/chat/completions', {
      //   model: request.options.model || 'gpt-3.5-turbo',
      //   messages: request.messages,
      //   temperature: request.options.temperature,
      //   max_tokens: request.options.maxTokens,
      //   top_p: request.options.topP,
      //   frequency_penalty: request.options.frequencyPenalty,
      //   presence_penalty: request.options.presencePenalty,
      //   stop: request.options.stop,
      //   stream: true
      // });
    } catch (error) {
      console.error('Chat streaming failed:', error);
      throw error;
    }
  },

  /**
   * ì±„íŒ… ìš”ì²­ ì¤‘ë‹¨
   */
  async stop(requestId: string): Promise<void> {
    try {
      await springAIClient.post(`/chat/stop/${requestId}`);
    } catch (error) {
      console.error('Chat stop failed:', error);
      throw error;
    }
  },

  /**
   * ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async getModels(): Promise<string[]> {
    try {
      // ëª¨ì˜ ë°ì´í„°
      return [
        'gpt-4',
        'gpt-3.5-turbo', 
        'claude-3-sonnet',
        'claude-3-haiku',
        'gemini-pro',
        'ollama/llama2',
        'ollama/mistral'
      ];
      
      // ì‹¤ì œ êµ¬í˜„
      // const response = await springAIClient.get<{ models: string[] }>('/models');
      // return response.models;
    } catch (error) {
      console.error('Get models failed:', error);
      throw error;
    }
  },

  /**
   * ëª¨ë¸ ì •ë³´ ì¡°íšŒ
   */
  async getModelInfo(modelId: string): Promise<any> {
    try {
      return await springAIClient.get(`/models/${encodeURIComponent(modelId)}`);
    } catch (error) {
      console.error('Get model info failed:', error);
      throw error;
    }
  }
};

/**
 * ëª¨ì˜ ì±„íŒ… ì™„ì„± ì‘ë‹µ (ë°±ì—”ë“œ ì¤€ë¹„ ì „ê¹Œì§€ ì‚¬ìš©)
 */
async function mockChatComplete(request: ChatRequest): Promise<ChatResponse> {
  // ì§€ì—° ì‹œë®¬ë ˆì´ì…˜
  await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 1200));

  const lastUserMessage = request.messages.filter(msg => msg.role === 'user').pop();
  const prompt = lastUserMessage?.content || 'No user message found';
  const model = request.options.model || 'gpt-3.5-turbo';

  // í”„ë¡¬í”„íŠ¸ì— ë”°ë¥¸ ë‹¤ì–‘í•œ ì‘ë‹µ ìƒì„±
  let responseContent: string;
  
  if (prompt.toLowerCase().includes('error') || prompt.toLowerCase().includes('ì—ëŸ¬')) {
    responseContent = `âŒ **ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ**\n\nìš”ì²­í•˜ì‹  "${prompt}"ì— ëŒ€í•´ ì—ëŸ¬ ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n\n**í•´ê²° ë°©ë²•:**\n1. ì…ë ¥ê°’ ê²€ì¦\n2. ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€\n3. ë¡œê·¸ í™•ì¸\n\n*ì´ê²ƒì€ Spring AI ChatClientë¥¼ í†µí•œ ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤.*`;
  } else if (prompt.toLowerCase().includes('code') || prompt.toLowerCase().includes('ì½”ë“œ')) {
    responseContent = `ğŸ’» **ì½”ë”© ì˜ˆì œ**\n\n\`\`\`typescript\n// Spring AI ì‚¬ìš© ì˜ˆì œ\nconst client = useChatClient();\n\nconst response = await client\n  .prompt("${prompt}")\n  .model("${model}")\n  .temperature(0.7)\n  .call();\n\nconsole.log(response);\n\`\`\`\n\n*ì´ê²ƒì€ Spring AI ChatClientë¥¼ í†µí•œ ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤.*`;
  } else if (prompt.toLowerCase().includes('explain') || prompt.toLowerCase().includes('ì„¤ëª…')) {
    responseContent = `ğŸ“š **ìƒì„¸ ì„¤ëª…**\n\n"${prompt}"ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤:\n\n## ì£¼ìš” ë‚´ìš©\n- **ê°œë…**: Spring AIëŠ” AI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤\n- **íŠ¹ì§•**: ë‹¤ì–‘í•œ AI ëª¨ë¸ ì§€ì›, íƒ€ì… ì•ˆì „ì„±, í™•ì¥ì„±\n- **í™œìš©**: ì±—ë´‡, ë¬¸ì„œ ê²€ìƒ‰, ìë™ ìš”ì•½ ë“±\n\n## ì¥ì \nâœ… ì¼ê´€ëœ API ì¸í„°í˜ì´ìŠ¤\nâœ… ì—¬ëŸ¬ AI ì œê³µì—…ì²´ ì§€ì›\nâœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›\n\n*ëª¨ë¸: ${model} | ëª¨ì˜ ì‘ë‹µ*`;
  } else {
    responseContent = `ğŸ¤– **Spring AI ì‘ë‹µ**\n\n"${prompt}"ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\nì´ê²ƒì€ **Spring AI ChatClient**ë¥¼ í†µí•´ ìƒì„±ëœ ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤. ì‹¤ì œ AI ë°±ì—”ë“œê°€ ì—°ê²°ë˜ë©´ ì‹¤ì œ AI ëª¨ë¸ì˜ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.\n\n**í˜„ì¬ ì„¤ì •:**\n- ëª¨ë¸: ${model}\n- ì˜¨ë„: ${request.options.temperature || 0.7}\n- ìµœëŒ€ í† í°: ${request.options.maxTokens || 'auto'}\n\n*ë” ìì„¸í•œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 'error', 'code', 'explain' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ë³´ì„¸ìš”.*`;
  }

  return {
    id: `chatcmpl-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
    object: 'chat.completion',
    created: Math.floor(Date.now() / 1000),
    model: model,
    choices: [{
      index: 0,
      message: {
        role: 'assistant',
        content: responseContent
      },
      finishReason: 'stop'
    }],
    usage: {
      promptTokens: Math.floor(prompt.length / 4) + Math.floor(Math.random() * 50),
      completionTokens: Math.floor(responseContent.length / 4) + Math.floor(Math.random() * 50),
      totalTokens: 0 // ìœ„ì—ì„œ ê³„ì‚°ëœ ê°’ë“¤ì˜ í•©
    }
  };
}

/**
 * ëª¨ì˜ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ë°±ì—”ë“œ ì¤€ë¹„ ì „ê¹Œì§€ ì‚¬ìš©)
 */
async function* mockChatStream(request: ChatRequest): AsyncIterableIterator<ChatStreamChunk> {
  const lastUserMessage = request.messages.filter(msg => msg.role === 'user').pop();
  const prompt = lastUserMessage?.content || 'No user message found';
  const model = request.options.model || 'gpt-3.5-turbo';
  
  const fullResponse = `ğŸŒŠ **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**

"${prompt}"ì— ëŒ€í•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì…ë‹ˆë‹¤.

ì´ê²ƒì€ Spring AI ChatClientì˜ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ ì‹œì—°í•˜ëŠ” ëª¨ì˜ ì‘ë‹µì…ë‹ˆë‹¤. ì‹¤ì œ AI ë°±ì—”ë“œê°€ ì—°ê²°ë˜ë©´ ì‹¤ì œ í† í°ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°ë©ë‹ˆë‹¤.

**ìŠ¤íŠ¸ë¦¬ë° ì¥ì :**
âœ¨ ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ
ğŸš€ ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜  
âš¡ ë¹ ë¥¸ ì²« í† í° ì‘ë‹µ

ëª¨ë¸: ${model} | ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ`;

  const words = fullResponse.split(' ');
  const requestId = `chatcmpl-${Date.now()}-stream`;

  for (let i = 0; i < words.length; i++) {
    // ë‹¨ì–´ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
    await new Promise(resolve => setTimeout(resolve, 50 + Math.random() * 100));
    
    const chunk: ChatStreamChunk = {
      id: requestId,
      object: 'chat.completion.chunk',
      created: Math.floor(Date.now() / 1000),
      model: model,
      choices: [{
        index: 0,
        delta: {
          content: (i === 0 ? words[i] : ' ' + words[i])
        },
        finishReason: i === words.length - 1 ? 'stop' : undefined
      }]
    };

    yield chunk;
  }
}

export default chatEndpoints;